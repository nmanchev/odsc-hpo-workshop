{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b14c7b9-2489-42ca-93a6-cc4d4b2451bc",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimisation Lab - 3\n",
    "\n",
    "The [HyperOpt](https://github.com/hyperopt/hyperopt) package, developed with support from leading government, academic and private institutions, offers a promising and easy-to-use implementation of a Bayesian hyperparameter optimization algorithm. In this notebook, we will cover how to perform hyperparameter optimization using a sequential model-based optimization (SMBO) technique implemented in the HyperOpt Python package.\n",
    "\n",
    "Sequential model-based optimization is a Bayesian optimization technique that uses information from past trials to inform the next set of hyperparameters to explore, and there are two variants of this algorithm used in practice:one based on the Gaussian process and the other on the Tree Parzen Estimator.\n",
    "\n",
    "We start by importing all packages needed by the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea780823-d212-45ac-89ac-a285583345c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is NOT available. Using CPU for training.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from mnist_cnn import load_mnist\n",
    "from mnist_cnn import Net\n",
    "from mnist_cnn import train\n",
    "from mnist_cnn import test\n",
    "from datetime import timedelta\n",
    "\n",
    "from hyperopt import tpe\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt import Trials\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin\n",
    "\n",
    "# This degrades the performance but enforces reproducibility.\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Note: This notebook hasn't been fully tested with GPU backends.\n",
    "# If you are using GPU-acceleration you may also have to enable the following lines:\n",
    "#\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "#torch.cuda.manual_seed_all(1234)\n",
    "\n",
    "# Make sure our tests are reproducible\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "  print(\"CUDA available. Using GPU acceleration.\\n\")\n",
    "  device = \"cuda\"\n",
    "else:\n",
    "  print(\"CUDA is NOT available. Using CPU for training.\\n\")\n",
    "  device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2de0c7-af67-4c73-a718-6800fc5baeb3",
   "metadata": {},
   "source": [
    "Loading the training, validation and test sets. Note, that we are only loading the first 20,000 samples from MNIST. This is not a big problem as the samples are relatively balanced across the 10 possible classes. If you'd like to stay on the safe side, you can use the following code to double check the distribution of classes:\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_y.numpy(), bins=10)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c3366e0-80ff-4679-bf66-c8a5ffc0e896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X size :  torch.Size([48000, 1, 28, 28])\n",
      "train_y size :  torch.Size([48000])\n",
      "valid_X size :  torch.Size([12000, 1, 28, 28])\n",
      "valid_y size :  torch.Size([12000])\n",
      "test_X size  :  torch.Size([10000, 1, 28, 28])\n",
      "test_y size  :  torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, valid_X, valid_y, test_X, test_y = load_mnist(device)\n",
    "\n",
    "train_X = train_X[:20000]\n",
    "train_y = train_y[:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f63b0b-8f8d-4ead-900c-22868e47dc0f",
   "metadata": {},
   "source": [
    "Next, we create the CNN network that we'll be fitting on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a64806-a236-4f39-aa4e-25b75af600e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc): Linear(in_features=1568, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net(activ = F.relu)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e142dfbd-ea5a-41ef-acb6-1564f61a38c6",
   "metadata": {},
   "source": [
    "HyperOpt requires the following key information to execute a hyperparameter search:\n",
    "* **objective function** - a function that the search will try to minimise\n",
    "* **search space** - they hyperparameter space to search over\n",
    "* **data store** - location to persist the search results\n",
    "* **search algorithm** - a search algorithm for selection of hyperparameter combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaacd063-0933-4a9e-9da5-6e8b8c491b58",
   "metadata": {},
   "source": [
    "Let's start by defining the **search space**. A search space consists of nested function expressions, including stochastic expressions. The stochastic expressions are the hyperparameters. The hyperparameter optimization algorithms work by replacing normal \"sampling\" logic with adaptive exploration strategies. Stochastic expressions normally have the form of \n",
    "\n",
    "```\n",
    "from hyperopt import hp\n",
    "\n",
    "hp.<sampling_type>(<label>, <range>)\n",
    "```\n",
    "\n",
    "Some commonly used stochastic expressions are:\n",
    "\n",
    "* hp.choice(label, options) --- Returns one of the options, which should be a list or tuple.\n",
    "\n",
    "* hp.randint(label, upper) --- Returns a random integer in the range [0, upper). The semantics of this distribution is that there is no more correlation in the loss function between nearby integer values, as compared with more distant integer values. This is an appropriate distribution for describing random seeds for example. \n",
    "\n",
    "* hp.uniform(label, low, high) --- Returns a value uniformly between low and high. When optimizing, this variable is constrained to a two-sided interval.\n",
    "\n",
    "* hp.normal(label, mu, sigma) --- Returns a real value that's normally-distributed with mean mu and standard deviation sigma. When optimizing, this is an unconstrained variable.\n",
    "\n",
    "For example, a bounded range from -10 to +10 for a hyperparameter named \"x\" can be describe with a search space:\n",
    "\n",
    "```\n",
    "space = hp.uniform('x', -10, 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6baca8-be6f-405b-936c-3d9cdc667ce6",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Define the following space:\n",
    "\n",
    "| Hyperparameter name | Sampling strategy                |\n",
    "| ------------------- | -------------------------------- |\n",
    "| learning rate       | uniform [0.0001, 0.1]            |\n",
    "| batch size          | one of [10, 20, 50, 100]         |\n",
    "| momentum            | constant [0.9]                   |\n",
    "| optimizer           | constant [optim.SGD]             |\n",
    "| loss_crierion       | constant [nn.CrossEntropyLoss()] |\n",
    "\n",
    "\n",
    "All hyperparameters should be packaged in a dictionary object like this:\n",
    "\n",
    "```\n",
    "search_space = { \"<hyperparameter1_name>\" : hp.<sampling_type>(\"<hyperparameter1_name>\", <range>),\n",
    "                 \"<hyperparameter2_name>\" : hp.<sampling_type>(\"<hyperparameter2_name>\", <range>),\n",
    "                 ...\n",
    "               }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0679dd2c-02e2-4d84-93f9-ba608f99e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\"learning_rate\" : hp.uniform(\"learning_rate\", 0.0001, 0.1),\n",
    "                \"batch_size\"    : hp.choice(\"batch_size\", [10, 20, 50, 100]),\n",
    "                \"momentum\"      : 0.9,\n",
    "                \"optimizer\"     : optim.SGD,\n",
    "                \"loss_crierion\" : nn.CrossEntropyLoss()\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ce14b-ea0d-45e8-bc0e-6eb0f61d26cb",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Each iteration in HyperOpt includes a call to the **objective function**. The function receives a point (a combination of parameters) from the search space and returns a loss (negative utility), which HyperOpt tries to minimise.\n",
    "\n",
    "Your next task is to define the objective function for optimising the hyperparameters of our CNN. While working on it, keep the following in mind:\n",
    "\n",
    "* All the hyperparameters (including the optimiser and the loss_criterion) are provided to the function via the *params* dictionary\n",
    "* The function should return the following dictionary as a result of its execution:\n",
    "\n",
    "```\n",
    "{ loss : <loss_on_the_validation_set>,\n",
    "  params : <parameters_used_for_training_the_CNN>,\n",
    "  status : STATUS_OK\n",
    "}\n",
    "```\n",
    "* Use the *train(\\<net\\>, \\<batch_size\\>, \\<loss_criterion\\>, \\<optimizer\\>, \\<train_X\\>, \\<train_y\\>, verbose = False)* to train the network. Here we set *verbose* to False to reduce the amount of information the network outputs during training.\n",
    "\n",
    "* To measure the accuracy on the validation set you can use the function \n",
    "\n",
    "```\n",
    "accuracy = test(<net>, valid_X, valid_y)\n",
    "```\n",
    "\n",
    "* You may want to print the validation accuracy after each function call so that you have a sense of how the optimisation is progressing\n",
    "\n",
    "* Don't forget to reset the network with each function call or your results won't be reproducible (use *net.reset()*)\n",
    "\n",
    "**NOTE**: \n",
    "1. Remember that HyperOpt will try to **minimise** the objective. In this case we are using it on the validation accuracy, which we want to maximise instead. You'll have to correct for this.\n",
    "2. Remember that some of the parameters passed to the **objective function** are optimiser parameters (you'll have to construct a proper optimiser object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae0e3a3c-acf6-4fbf-95d0-5bffef046c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    \n",
    "    net.reset()\n",
    "    \n",
    "    batch_size = params[\"batch_size\"]\n",
    "    loss_criterion = params[\"loss_crierion\"]\n",
    "    \n",
    "    optimizer = params[\"optimizer\"](net.parameters(), lr=params[\"learning_rate\"], momentum=params[\"momentum\"])\n",
    "    \n",
    "    loss_hist = train(net, batch_size, loss_criterion, optimizer, train_X, train_y, verbose = False)\n",
    "    \n",
    "    \n",
    "    val_acc = test(net, valid_X, valid_y)\n",
    "    \n",
    "    print(\"Run completed. Validation accuracy: {:.4f}\".format(val_acc))\n",
    "    print(20*\"-\")\n",
    "    \n",
    "    return {\"loss\" : -val_acc, \"params\" : params, \"status\" : STATUS_OK} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981fb8bc-b91a-4e44-a2a7-a73452566ed2",
   "metadata": {},
   "source": [
    "The final two elements that we need to define are the **search algorithm** and the **data store**.\n",
    "\n",
    "HyperOpt currently supports three search algorithms:\n",
    "\n",
    "* Random Search (hyperopt.random.suggest) --- [relevant paper](http://www.jmlr.org/papers/v13/bergstra12a.html)\n",
    "* Tree of Parzen Estimators (TPE) (hyperopt.tpe.suggest) --- [relevant paper](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)\n",
    "* Adaptive TPE (ATPE) (hyperopt.atpe.suggest) --- [relevant paper](https://journals.sagepub.com/doi/pdf/10.1177/0020294020932347)\n",
    "\n",
    "In this tutorial we will use *hyperopt.tpe.suggest* so let's set it as our search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ba24dad-8508-4ac5-8269-349b24c74dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = tpe.suggest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6f1d24-c65d-4cee-adbf-76c992f42bfb",
   "metadata": {},
   "source": [
    "HyperOpt provides two storage mechanisms:\n",
    "\n",
    "* Trials --- a pure-Python implementation a the storage database using lists of dictionaries.\n",
    "* MongoTrials --- implements the same API in terms of a mongodb database running in another process.\n",
    "\n",
    "The elements of *Trials* represent all of the completed, in-progress, and scheduled evaluation points from an e.g. `fmin` call.\n",
    "\n",
    "Each element a dictionary with *at least* the following keys:\n",
    "\n",
    "* **tid**: a unique trial identification object within this Trials instance usually it is an integer, but it isn't obvious that other sortable, hashable objects couldn't be used at some point.\n",
    "      \n",
    "* **result**: a sub-dictionary representing what was returned by the fmin evaluation function. This sub-dictionary has a key 'status' with a value from `STATUS_STRINGS` and the status is `STATUS_OK`, then there should be a 'loss' key as well with a floating-point value.  \n",
    "\n",
    "* **misc**: despite generic name, this is currently where the trial's hyperparameter assignments are stored.\n",
    "\n",
    "In this tutorial we'll use the *Trials* database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56cb4744-5aad-4f67-8ea2-4e1630637eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_trials = Trials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6275866-30fe-48ab-a5c7-083b5bb8db21",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Now that we have all prerequisites in place, we can finally kick off the optimisation process via a call to the *fmin* function. *fmin* explores a function over a hyperparameter space according to a given algorithm. The key arguments that we need to provide are:\n",
    "\n",
    "* **objecitve** --- This function is called with a value generated from **search** as the first and possibly only argument.  It can return either a scalar-valued loss, or a dictionary.  A returned dictionary must contain a 'status' key with a value from `STATUS_STRINGS`, must contain a 'loss' key if the status is `STATUS_OK`. In our case the **objecitve** is simply called *objective* (This is the function we defined in Task 2).\n",
    "\n",
    "* **space** --- The set of possible arguments to **objective**. In our case this is *search_space* (Task 1)\n",
    "\n",
    "* **algo** --- This object, such as `hyperopt.rand.suggest` and `hyperopt.tpe.suggest` provides logic for sequential search of the hyperparameter space. We will use the *algorithm* variable here.\n",
    "\n",
    "* **max_evals** --- Allow up to this many function evaluations before returning. We will use the value 5 here.\n",
    "\n",
    "* **trials** --- Storage for completed, ongoing, and scheduled evaluation points. We will use *bayes_trials*.\n",
    "\n",
    "* **rstate** --- Each call to **algo** requires a seed value, which should be different on each call. Here we'll clamp it to ```np.random.default_rng(1234)``` to ensure reproducibility.\n",
    "\n",
    "OK, let's now run *fmin* and store its output in a variable called *best_params*. Use the code cell below to make the function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cab66c7d-6d03-42cf-8530-d9b4b31a1b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed. Validation accuracy: 0.0962            \n",
      "--------------------                                  \n",
      "Run completed. Validation accuracy: 0.0990                                        \n",
      "--------------------                                                              \n",
      "Run completed. Validation accuracy: 0.0990                                        \n",
      "--------------------                                                             \n",
      "Run completed. Validation accuracy: 0.0990                                       \n",
      "--------------------                                                             \n",
      "Run completed. Validation accuracy: 0.0962                                       \n",
      "--------------------                                                             \n",
      "Run completed. Validation accuracy: 0.0962                                       \n",
      "--------------------                                                             \n",
      "Run completed. Validation accuracy: 0.0990                                       \n",
      "--------------------                                                             \n",
      "Run completed. Validation accuracy: 0.0990                                       \n",
      "--------------------                                                             \n",
      "Run completed. Validation accuracy: 0.0990                                       \n",
      "--------------------                                                             \n",
      "Run completed. Validation accuracy: 0.0990                                       \n",
      "--------------------                                                             \n",
      "Run completed. Validation accuracy: 0.0990                                        \n",
      "--------------------                                                              \n",
      "Run completed. Validation accuracy: 0.0962                                        \n",
      "--------------------                                                              \n",
      "Run completed. Validation accuracy: 0.0962                                        \n",
      "--------------------                                                              \n",
      "Run completed. Validation accuracy: 0.9592                                        \n",
      "--------------------                                                              \n",
      "Run completed. Validation accuracy: 0.0962                                        \n",
      "--------------------                                                              \n",
      "Run completed. Validation accuracy: 0.0962                                        \n",
      "--------------------                                                              \n",
      "Run completed. Validation accuracy: 0.0990                                        \n",
      "--------------------                                                              \n",
      "Run completed. Validation accuracy: 0.0990                                        \n",
      "--------------------                                                              \n",
      "Run completed. Validation accuracy: 0.0990                                        \n",
      "--------------------                                                              \n",
      "Run completed. Validation accuracy: 0.0962                                        \n",
      "--------------------                                                              \n",
      "100%|██████████| 20/20 [06:30<00:00, 19.54s/trial, best loss: -0.9592499732971191]\n"
     ]
    }
   ],
   "source": [
    "best_params = fmin(objective, search_space, algo=algorithm, max_evals=20, trials=bayes_trials, rstate=np.random.default_rng(1234))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c42181f-c6dc-49e6-bb06-275c55d07799",
   "metadata": {},
   "source": [
    "Now that the search is complete, we can inspect our **data store** and see all the trails and the corresponding results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3befb32e-86e7-49b8-9d3f-a44844f2523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': 2, 'tid': 0, 'spec': None, 'result': {'loss': -0.09616667032241821, 'params': {'batch_size': 50, 'learning_rate': 0.021123581053336123, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 0, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [0], 'learning_rate': [0]}, 'vals': {'batch_size': [2], 'learning_rate': [0.021123581053336123]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 39, 3, 242000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 39, 15, 885000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 1, 'spec': None, 'result': {'loss': -0.0989999994635582, 'params': {'batch_size': 20, 'learning_rate': 0.07618179083703541, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 1, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [1], 'learning_rate': [1]}, 'vals': {'batch_size': [1], 'learning_rate': [0.07618179083703541]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 39, 15, 888000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 39, 32, 393000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 2, 'spec': None, 'result': {'loss': -0.0989999994635582, 'params': {'batch_size': 10, 'learning_rate': 0.06858505473887806, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 2, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [2], 'learning_rate': [2]}, 'vals': {'batch_size': [0], 'learning_rate': [0.06858505473887806]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 39, 32, 395000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 39, 57, 814000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 3, 'spec': None, 'result': {'loss': -0.0989999994635582, 'params': {'batch_size': 20, 'learning_rate': 0.0732167979191985, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 3, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [3], 'learning_rate': [3]}, 'vals': {'batch_size': [1], 'learning_rate': [0.0732167979191985]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 39, 57, 817000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 40, 14, 183000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 4, 'spec': None, 'result': {'loss': -0.09616667032241821, 'params': {'batch_size': 100, 'learning_rate': 0.07817767153249514, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 4, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [4], 'learning_rate': [4]}, 'vals': {'batch_size': [3], 'learning_rate': [0.07817767153249514]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 40, 14, 185000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 40, 28, 882000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 5, 'spec': None, 'result': {'loss': -0.09616667032241821, 'params': {'batch_size': 20, 'learning_rate': 0.023843025639959912, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 5, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [5], 'learning_rate': [5]}, 'vals': {'batch_size': [1], 'learning_rate': [0.023843025639959912]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 40, 28, 884000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 40, 45, 483000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 6, 'spec': None, 'result': {'loss': -0.0989999994635582, 'params': {'batch_size': 10, 'learning_rate': 0.032747888038415815, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 6, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [6], 'learning_rate': [6]}, 'vals': {'batch_size': [0], 'learning_rate': [0.032747888038415815]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 40, 45, 486000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 41, 9, 914000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 7, 'spec': None, 'result': {'loss': -0.0989999994635582, 'params': {'batch_size': 10, 'learning_rate': 0.09754835261849307, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 7, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [7], 'learning_rate': [7]}, 'vals': {'batch_size': [0], 'learning_rate': [0.09754835261849307]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 41, 9, 917000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 41, 37, 685000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 8, 'spec': None, 'result': {'loss': -0.0989999994635582, 'params': {'batch_size': 10, 'learning_rate': 0.07901191386279659, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 8, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [8], 'learning_rate': [8]}, 'vals': {'batch_size': [0], 'learning_rate': [0.07901191386279659]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 41, 37, 688000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 42, 3, 86000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 9, 'spec': None, 'result': {'loss': -0.0989999994635582, 'params': {'batch_size': 10, 'learning_rate': 0.057266530262351095, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 9, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [9], 'learning_rate': [9]}, 'vals': {'batch_size': [0], 'learning_rate': [0.057266530262351095]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 42, 3, 88000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 42, 28, 521000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 10, 'spec': None, 'result': {'loss': -0.0989999994635582, 'params': {'batch_size': 10, 'learning_rate': 0.058559616049386164, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 10, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [10], 'learning_rate': [10]}, 'vals': {'batch_size': [0], 'learning_rate': [0.058559616049386164]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 42, 28, 523000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 42, 54, 883000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 11, 'spec': None, 'result': {'loss': -0.09616667032241821, 'params': {'batch_size': 50, 'learning_rate': 0.03975524658993317, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 11, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [11], 'learning_rate': [11]}, 'vals': {'batch_size': [2], 'learning_rate': [0.03975524658993317]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 42, 54, 885000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 43, 10, 191000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 12, 'spec': None, 'result': {'loss': -0.09616667032241821, 'params': {'batch_size': 50, 'learning_rate': 0.025005644682654055, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 12, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [12], 'learning_rate': [12]}, 'vals': {'batch_size': [2], 'learning_rate': [0.025005644682654055]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 43, 10, 193000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 43, 22, 594000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 13, 'spec': None, 'result': {'loss': -0.9592499732971191, 'params': {'batch_size': 10, 'learning_rate': 0.00011394408377564594, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 13, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [13], 'learning_rate': [13]}, 'vals': {'batch_size': [0], 'learning_rate': [0.00011394408377564594]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 43, 22, 597000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 43, 46, 184000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 14, 'spec': None, 'result': {'loss': -0.09616667032241821, 'params': {'batch_size': 100, 'learning_rate': 0.07910676181949454, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 14, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [14], 'learning_rate': [14]}, 'vals': {'batch_size': [3], 'learning_rate': [0.07910676181949454]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 43, 46, 186000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 43, 57, 203000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 15, 'spec': None, 'result': {'loss': -0.09616667032241821, 'params': {'batch_size': 50, 'learning_rate': 0.02977266614040135, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 15, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [15], 'learning_rate': [15]}, 'vals': {'batch_size': [2], 'learning_rate': [0.02977266614040135]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 43, 57, 206000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 44, 13, 389000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 16, 'spec': None, 'result': {'loss': -0.0989999994635582, 'params': {'batch_size': 20, 'learning_rate': 0.07907949309802645, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 16, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [16], 'learning_rate': [16]}, 'vals': {'batch_size': [1], 'learning_rate': [0.07907949309802645]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 44, 13, 391000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 44, 29, 386000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 17, 'spec': None, 'result': {'loss': -0.0989999994635582, 'params': {'batch_size': 10, 'learning_rate': 0.02552006417485902, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 17, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [17], 'learning_rate': [17]}, 'vals': {'batch_size': [0], 'learning_rate': [0.02552006417485902]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 44, 29, 388000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 44, 53, 194000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 18, 'spec': None, 'result': {'loss': -0.0989999994635582, 'params': {'batch_size': 10, 'learning_rate': 0.036257362251136516, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 18, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [18], 'learning_rate': [18]}, 'vals': {'batch_size': [0], 'learning_rate': [0.036257362251136516]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 44, 53, 197000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 45, 16, 788000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 19, 'spec': None, 'result': {'loss': -0.09616667032241821, 'params': {'batch_size': 50, 'learning_rate': 0.06392787676372762, 'loss_crierion': CrossEntropyLoss(), 'momentum': 0.9, 'optimizer': <class 'torch.optim.sgd.SGD'>}, 'status': 'ok'}, 'misc': {'tid': 19, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [19], 'learning_rate': [19]}, 'vals': {'batch_size': [2], 'learning_rate': [0.06392787676372762]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 45, 16, 790000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 45, 33, 995000)}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for trial in bayes_trials:\n",
    "    print(trial)\n",
    "    print(20*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf14942e-d95c-49f4-a086-0fdf68691914",
   "metadata": {},
   "source": [
    "There are two ways of fetching the best performing run (i.e. the one with the lowest loss). We can either inspect the value returned by *fmin* directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70e5a147-ec6e-4bbd-b453-ef2a3209719d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 0, 'learning_rate': 0.00011394408377564594}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9694412e-2550-4222-bbef-9eb1aca1d9af",
   "metadata": {},
   "source": [
    "Or we can query the **data store** for specific values in the best performing trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51dbc992-4782-4cea-a106-48e4295ae311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9592499732971191"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_trials.best_trial[\"result\"][\"loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2c9524-6cd7-49ea-9caa-27076abfcd5d",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Now that we have the best hyperparameters, we need to make sure that the results are reprodicible. Inspect the values in *best_params*, reset and retrain the network. The resulting validation accuracy should be identical to the value stored in ```bayes_trials.best_trial[\"result\"][\"loss\"]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aecf3e1e-3887-473c-be25-daa1c2af7531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs [1/1], Samples[1000/20000], Loss: 20.0983\n",
      "Epochs [1/1], Samples[2000/20000], Loss: 1.2243\n",
      "Epochs [1/1], Samples[3000/20000], Loss: 0.5176\n",
      "Epochs [1/1], Samples[4000/20000], Loss: 0.9346\n",
      "Epochs [1/1], Samples[5000/20000], Loss: 0.1125\n",
      "Epochs [1/1], Samples[6000/20000], Loss: 0.6413\n",
      "Epochs [1/1], Samples[7000/20000], Loss: 0.9878\n",
      "Epochs [1/1], Samples[8000/20000], Loss: 0.0091\n",
      "Epochs [1/1], Samples[9000/20000], Loss: 0.0195\n",
      "Epochs [1/1], Samples[10000/20000], Loss: 0.2065\n",
      "Epochs [1/1], Samples[11000/20000], Loss: 0.2222\n",
      "Epochs [1/1], Samples[12000/20000], Loss: 0.0680\n",
      "Epochs [1/1], Samples[13000/20000], Loss: 0.0171\n",
      "Epochs [1/1], Samples[14000/20000], Loss: 0.0178\n",
      "Epochs [1/1], Samples[15000/20000], Loss: 0.0091\n",
      "Epochs [1/1], Samples[16000/20000], Loss: 0.0015\n",
      "Epochs [1/1], Samples[17000/20000], Loss: 0.0016\n",
      "Epochs [1/1], Samples[18000/20000], Loss: 0.0089\n",
      "Epochs [1/1], Samples[19000/20000], Loss: 0.2108\n",
      "Epochs [1/1], Samples[20000/20000], Loss: 0.0695\n",
      "Run completed. Validation accuracy: 0.9592499733\n"
     ]
    }
   ],
   "source": [
    "net.reset()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=best_params[\"learning_rate\"], momentum = 0.9)    \n",
    "loss_hist = train(net, 10, nn.CrossEntropyLoss(), optimizer, train_X, train_y, verbose = True)\n",
    "    \n",
    "acc = test(net, valid_X, valid_y)\n",
    "print(\"Run completed. Validation accuracy: {:.10f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55cf3f0-cb47-4677-b719-f89b38206bcb",
   "metadata": {},
   "source": [
    "Do the accuracieis match? Compare the values of \"loss\" from *best_trials* and the validation that we've just computed. Remember, that the objective returns the negative accuracy so we need to correct for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f12cd90e-3b7f-41d2-aa56-184903c0d983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print((bayes_trials.best_trial[\"result\"][\"loss\"] == -acc).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8e484-720f-4d6d-bb90-e44d922a3ab7",
   "metadata": {},
   "source": [
    "## Slightly more advanced exploration \n",
    "\n",
    "HyperOpt provides mechanisms for defining adaptive search spaces. This is useful when there are internal dependencies in terms of ranges (or presence) of search space hyperparameters. For example, the algorithm you are fitting can itself be a parameter of the search space. You could have a search space hyperparameter that looks like this:\n",
    "\n",
    "```\n",
    "space = hp.choice(\"classifier_type\", [\n",
    "                     {\n",
    "                        \"type\": \"svm\"\n",
    "                     },\n",
    "                     {\n",
    "                        \"type\": \"dtree\"\n",
    "                     }\n",
    "])\n",
    "```\n",
    "\n",
    "The issue here is that if you define your search space like this the specific hyperparameters now have a dependency on *classifier_type* (i.e. it doesn't make sense to search for the best value of *max_depth* if you are fitting an SVM model. There are two ways to handle this:\n",
    "\n",
    "* **Option 1**: Define your search space based on the algorithm. You are essentially looking for a statement that implements the following logic:\n",
    "\n",
    "```\n",
    " if classifier_type == \"svm\":\n",
    "        \n",
    "        search_space = {\n",
    "            \"C\": hp.lognormal(\"svm_C\", 0, 1),\n",
    "            \"kernel\": hp.choice(\"svm_kernel\", [\n",
    "                {\"ktype\": \"linear\"},\n",
    "                {\"ktype\": \"RBF\", \"width\": hp.lognormal(\"svm_rbf_width\", 0, 1)}])\n",
    "            }\n",
    "        \n",
    "elif classifier_type == \"dtree\":\n",
    "        search_space = {\n",
    "            \"criterion\": hp.choice(\"dtree_criterion\", [\"gini\", \"entropy\"]),\n",
    "            \"min_samples_split\": hp.qlognormal(\"dtree_min_samples_split\", 2, 1, 1)\n",
    "        }\n",
    "```\n",
    "\n",
    "* **Option 2**: You nest the classifier specific hyperparameters like this:\n",
    "\n",
    "```\n",
    "search_space = hp.choice(\"classifier_type\", [\n",
    "        {\n",
    "            \"type\": \"svm\",\n",
    "            \"C\": hp.lognormal(\"svm_C\", 0, 1),\n",
    "            \"kernel\": hp.choice(\"svm_kernel\", [\n",
    "                {\"ktype\": \"linear\"},\n",
    "                {\"ktype\": \"RBF\", \"width\": hp.lognormal(\"svm_rbf_width\", 0, 1)},\n",
    "                ]),\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"dtree\",\n",
    "            \"criterion\": hp.choice(\"dtree_criterion\", [\"gini\", \"entropy\"]),\n",
    "            \"min_samples_split\": hp.qlognormal(\"dtree_min_samples_split\", 2, 1, 1),\n",
    "        }\n",
    "    ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e90c518-1ff3-4291-b3d6-4aa4e8bee8a7",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "In this task you'll define a search space that has the optimiser as a hyperparameter:\n",
    "\n",
    "* HyperOpt will have to chose between two options --- optim.SGD and optim.Adagrad.\n",
    "* optim.SGD should have a *momentum* parameter with possible values in [0, 0.5, 0.9]\n",
    "\n",
    "Let's set up the search space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c129b59b-18e2-49bf-a2d8-bbefc6d02535",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\"learning_rate\" : hp.uniform(\"learning_rate\", 0.0001, 0.1),\n",
    "                \"batch_size\"    : hp.choice(\"batch_size\", [10, 20, 50, 100]),\n",
    "                \"optimizer\"     : hp.choice(\"optimizer\",[\n",
    "                                    {\"type\": optim.SGD,\n",
    "                                    \"momentum\"      : 0.9}, \n",
    "                                    {\"type\": optim.Adagrad}\n",
    "                                  ]),\n",
    "                \"loss_crierion\" : nn.CrossEntropyLoss()\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b1f070-de41-4767-998c-974007913cbe",
   "metadata": {},
   "source": [
    "Next, we need to configure the objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b443774f-c03f-45b2-942a-13053c761c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    \n",
    "    net.reset()\n",
    "    \n",
    "    batch_size = params[\"batch_size\"]\n",
    "    loss_criterion = params[\"loss_crierion\"]\n",
    "    \n",
    "    opt = params[\"optimizer\"]\n",
    "    \n",
    "    if (opt[\"type\"] == optim.SGD):\n",
    "        optimizer = opt[\"type\"](net.parameters(), lr=params[\"learning_rate\"], momentum=opt[\"momentum\"])\n",
    "    else:\n",
    "        optimizer = opt[\"type\"](net.parameters(), lr=params[\"learning_rate\"])\n",
    "    \n",
    "    loss_hist = train(net, batch_size, loss_criterion, optimizer, train_X, train_y, verbose = False)\n",
    "    \n",
    "    \n",
    "    val_acc = test(net, valid_X, valid_y)\n",
    "    \n",
    "    print(\"Run completed. Validation accuracy: {:.4f}\".format(val_acc))\n",
    "    print(20*\"-\")\n",
    "    \n",
    "    return {\"loss\" : -val_acc, \"params\" : params, \"status\" : STATUS_OK} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476024f-3367-4381-8581-2d27aa82351b",
   "metadata": {},
   "source": [
    "Finally, we call *fmin* to find the best set of hyperparameters from this extended search space. Note that we are now running 10 trials (to account for the two optimisers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "881beefc-688b-418e-a3bb-c43694f51441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed. Validation accuracy: 0.0990           \n",
      "--------------------                                 \n",
      "Run completed. Validation accuracy: 0.8593                                      \n",
      "--------------------                                                            \n",
      "Run completed. Validation accuracy: 0.9196                                      \n",
      "--------------------                                                           \n",
      "Run completed. Validation accuracy: 0.9522                                      \n",
      "--------------------                                                            \n",
      "Run completed. Validation accuracy: 0.7467                                      \n",
      "--------------------                                                            \n",
      "100%|██████████| 5/5 [01:21<00:00, 16.32s/trial, best loss: -0.9521666765213013]\n"
     ]
    }
   ],
   "source": [
    "bayes_trials = Trials()\n",
    "best_params = fmin(objective, search_space, algo=algorithm, max_evals=5, trials=bayes_trials, rstate=np.random.default_rng(1234))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3b4bb2-68e3-4ae5-8641-f51a134478cc",
   "metadata": {},
   "source": [
    "Now let's inspect the trials performed by HyperOpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60004af9-09a6-455c-b94c-4aa65db95082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': 2, 'tid': 0, 'spec': None, 'result': {'loss': -0.0989999994635582, 'params': {'batch_size': 10, 'learning_rate': 0.07953040550806391, 'loss_crierion': CrossEntropyLoss(), 'optimizer': {'momentum': 0.9, 'type': <class 'torch.optim.sgd.SGD'>}}, 'status': 'ok'}, 'misc': {'tid': 0, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [0], 'learning_rate': [0], 'optimizer': [0]}, 'vals': {'batch_size': [0], 'learning_rate': [0.07953040550806391], 'optimizer': [0]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 46, 12, 13000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 46, 35, 521000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 1, 'spec': None, 'result': {'loss': -0.859333336353302, 'params': {'batch_size': 100, 'learning_rate': 0.03655121639943982, 'loss_crierion': CrossEntropyLoss(), 'optimizer': {'type': <class 'torch.optim.adagrad.Adagrad'>}}, 'status': 'ok'}, 'misc': {'tid': 1, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [1], 'learning_rate': [1], 'optimizer': [1]}, 'vals': {'batch_size': [3], 'learning_rate': [0.03655121639943982], 'optimizer': [1]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 46, 35, 582000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 46, 48, 16000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 2, 'spec': None, 'result': {'loss': -0.9195833206176758, 'params': {'batch_size': 50, 'learning_rate': 0.06127515038517743, 'loss_crierion': CrossEntropyLoss(), 'optimizer': {'type': <class 'torch.optim.adagrad.Adagrad'>}}, 'status': 'ok'}, 'misc': {'tid': 2, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [2], 'learning_rate': [2], 'optimizer': [2]}, 'vals': {'batch_size': [2], 'learning_rate': [0.06127515038517743], 'optimizer': [1]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 46, 48, 18000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 47, 2, 8000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 3, 'spec': None, 'result': {'loss': -0.9521666765213013, 'params': {'batch_size': 50, 'learning_rate': 0.006160745469131047, 'loss_crierion': CrossEntropyLoss(), 'optimizer': {'type': <class 'torch.optim.adagrad.Adagrad'>}}, 'status': 'ok'}, 'misc': {'tid': 3, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [3], 'learning_rate': [3], 'optimizer': [3]}, 'vals': {'batch_size': [2], 'learning_rate': [0.006160745469131047], 'optimizer': [1]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 47, 2, 10000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 47, 17, 889000)}\n",
      "--------------------\n",
      "{'state': 2, 'tid': 4, 'spec': None, 'result': {'loss': -0.746749997138977, 'params': {'batch_size': 100, 'learning_rate': 0.014962477020368313, 'loss_crierion': CrossEntropyLoss(), 'optimizer': {'momentum': 0.9, 'type': <class 'torch.optim.sgd.SGD'>}}, 'status': 'ok'}, 'misc': {'tid': 4, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'batch_size': [4], 'learning_rate': [4], 'optimizer': [4]}, 'vals': {'batch_size': [3], 'learning_rate': [0.014962477020368313], 'optimizer': [0]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 5, 9, 18, 47, 17, 891000), 'refresh_time': datetime.datetime(2023, 5, 9, 18, 47, 33, 594000)}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for trial in bayes_trials:\n",
    "    print(trial)\n",
    "    print(20*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14080a0-dc95-4d97-9b8f-2cf9d46fff68",
   "metadata": {},
   "source": [
    "... and the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9ae10c6-2b45-4219-aa6d-d21cffd8d2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 2, 'learning_rate': 0.006160745469131047, 'optimizer': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a7528-d089-432e-aaea-3313279a6319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
